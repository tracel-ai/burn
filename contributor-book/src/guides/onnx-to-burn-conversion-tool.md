# ONNX to Burn: Development Guide

This guide offers in-depth design insights and step-by-step procedures for developers working on the
ONNX to Burn conversion tool. This tool allows the importation of ONNX models into the Burn deep
learning framework written in Rust. It converts both ONNX models to Rust source code and model
weights to Burn state files.

For an introduction to ONNX import in Burn, see
[this section of the Burn book](https://burn.dev/books/burn/import/onnx-model.html).

## Design Overview

### Design Goals

- Perform best-effort conversion of ONNX models to Rust source code via Burn APIs.
- Convert ONNX model weights to Burn state files.
- Support ONNX models generated by PyTorch (ONNX Opset 16+).
- Produce easy-to-understand and modifiable models.
- Ensure the generated models are trainable using Burn APIs.

### Design Decisions

**Core Principles:**

- **Op/Node-Centric Design**: Built around individual operations and nodes for better scalability as
  more operators are added
- **Opset-Aware Processing**: Processors accept opset parameters for flexible behavior across
  different ONNX versions
- **Constants-First Approach**: All ONNX initializers are treated as constant nodes initially,
  providing a uniform starting point
- **Native Type Integration**: Direct use of `burn_tensor::TensorData` and `Dtype` for efficiency,
  consistency, and future mmap support
- **Multi-Phase Pipeline**: Explicit transformation phases (initialization → conversion → type
  inference → post-processing → finalization) for better visibility and maintainability
- **Graph Input Name Preservation**: Sanitized ONNX names are preserved for easier development and
  troubleshooting

**Separation of Concerns:**

- Limit interaction with ONNX to the Intermediate Representation (IR) stage to simplify the process
- Ensure operator behavior consistency across different OpSet versions
- Exclude any ONNX/Protobuf-specific logic from the Burn graph

The conversion process involves three main stages:

1. Convert ONNX model to Intermediate Representation (IR) via 5-phase pipeline.
2. Translate IR to a Burn graph.
3. Generate Rust source code from the Burn graph.

## Adding New Operators

To extend `burn-import` with support for new ONNX operators, follow these steps:

1. **Create PyTorch Script**: Place a PyTorch script using the new operator under
   `crates/burn-import/onnx-tests/tests/<op>/<op>.py`. Make sure to print both input and output
   tensors for end-to-end testing.

2. **Generate ONNX Model**: Run the PyTorch script to produce an ONNX model.

3. **Visualize ONNX Model**: Use [Netron](https://github.com/lutzroeder/netron) to verify the ONNX
   model contains the expected operators.

4. **Generate IR and Burn Graph**: Navigate to
   [crates/burn-import/](https://github.com/tracel-ai/burn/tree/main/crates/burn-import) and run:

   ```
   cargo r -- ./onnx-tests/tests/<op>/<op>.onnx ./out
   ```

5. **Implement Missing Operators**: If you encounter an error stating that an operator is
   unsupported, [implement it](#implementing-a-new-operator). The `./out/my-model.graph.txt` should
   provide relevant information.

6. **Inspect Generated Files**: The `my-model.graph.txt` contains IR details, `my-model.rs` holds
   the Burn model in Rust code, and `my-model.json` includes the model data.

7. **Add End-to-End Test**: Include the test in
   [crates/burn-import/onnx-tests/tests/test_onnx.rs](https://github.com/tracel-ai/burn/blob/main/crates/burn-import/onnx-tests/tests/test_onnx.rs).
   Further details can be found in the
   [onnx-tests README](https://github.com/tracel-ai/burn/blob/main/crates/burn-import/onnx-tests/README.md).

## Implementing a New Operator

To extend the capabilities of the Burn library by supporting new operations imported from ONNX
graphs, developers must go through a few systematic steps. Here, we detail the process, using the
implementation of the `Squeeze` operation to illustrate points as needed. All file/directory paths
are relative to the root of the burn repository.

### Step 1: Node Processor Implementation in onnx-ir

The `onnx-ir` crate handles the Intermediate Representation (IR) of ONNX models using a
processor-based architecture. For each operation:

1. **Create a node module** in `crates/onnx-ir/src/node/<operation_name>.rs`. This file should
   contain:
   - **Configuration struct**: Define operation-specific parameters (e.g., `SqueezeConfig`)
   - **Processor struct**: Implement `NodeProcessor` trait (marked as `pub(crate)`)
   - The processor handles:
     - **Input/output specification**: Define expected inputs and outputs via `NodeSpec`
     - **Type inference**: Infer output types from inputs and configuration
     - **Configuration extraction**: Extract operation parameters from ONNX attributes
     - **Node construction**: Build the final `Node` enum variant with config

2. **Make the module visible** in `crates/onnx-ir/src/node/mod.rs`:

   ```rust
   pub mod squeeze;
   ```

3. **Add to the Node enum** in `crates/onnx-ir/src/ir/node.rs` by adding a variant to the
   `define_node_enum!` macro invocation:

   ```rust
   define_node_enum! {
       // ... other variants
       Squeeze { config: squeeze::SqueezeConfig },
   }
   ```

4. **Register your processor** in `crates/onnx-ir/src/registry.rs` by adding it to the
   `with_standard_processors()` function:
   ```rust
   registry.register("Squeeze", Box::new(squeeze::SqueezeProcessor));
   ```

For example, the squeeze operation in `crates/onnx-ir/src/node/squeeze.rs` contains:

- A `SqueezeConfig` struct with operation parameters (axes)
- A `SqueezeProcessor` struct (marked `pub(crate)`) that implements `NodeProcessor`
- The `node_spec()` method defines input/output requirements
- The `process()` method extracts config and constructs the `Node::Squeeze` variant

### Step 2: Node Implementation in burn-import

1. Create a new file named `<operation_name>.rs` in the `crates/burn-import/src/burn/node/`
   directory. This file will define the structure and functionality of your new operation. By
   convention, the necessary information for carrying out an operation is encapsulated within a
   struct named `<operation>Node`. For the `Squeeze` operation, we defined a struct called
   `SqueezeNode` that holds necessary information about the input tensor, output tensor, and axes
   for the operation.

2. Implement the `OnnxIntoNode` trait for your node. This trait has a single method `from_onnx` that
   converts an ONNX IR node into your Burn node type. Use pattern matching with destructuring to
   extract the config and inputs/outputs:

   ```rust
   impl OnnxIntoNode for SqueezeNode {
       fn from_onnx(node: onnx_ir::ir::Node) -> Self {
           // Pattern match on the Node enum variant
           let (inputs, outputs, config) = match node {
               onnx_ir::ir::Node::Squeeze { inputs, outputs, config, .. } => {
                   (inputs, outputs, config)
               }
               _ => panic!("Expected Squeeze node"),
           };

           let input = TensorType::from(inputs.first().unwrap());
           let output = TensorType::from(outputs.first().unwrap());

           SqueezeNode::new(input, output, config.axes)
       }
   }
   ```

3. The core of integrating a new operation involves implementing the `NodeCodegen` trait for your
   node. This trait defines how the node generates code during the graph compilation process. The
   implementation must provide methods to define input and output types, to generate the forward
   pass code, and to encapsulate the node into the more general `Node` structure. Specifically:
   - `input_types()` - Returns the types of all input arguments
   - `output_types()` - Returns the types of all output values
   - `forward()` - Generates the Rust code that performs the operation during execution. Use the
     `quote!` macro to generate code and ensure it's syntactically correct Burn code
   - `into_node()` - Wraps the node into the general `Node<PS>` enum

   Example implementation:

   ```rust
   impl<PS: PrecisionSettings> NodeCodegen<PS> for SqueezeNode {
       fn input_types(&self) -> Vec<Type> {
           vec![self.input.clone()]
       }

       fn output_types(&self) -> Vec<Type> {
           vec![self.output.clone()]
       }

       fn forward(&self, scope: &mut Scope, node_position: usize) -> TokenStream {
           // Simplified example - actual implementation handles more cases
           let input = scope.tensor_use_owned(&self.input, node_position);
           let output = &self.output.name();

           match &self.axes {
               Some(axes) => {
                   let axes_tokens = axes.to_tokens();
                   quote! {
                       let #output = #input.squeeze_dims(&#axes_tokens);
                   }
               }
               None => {
                   let output_rank = self.output.rank();
                   quote! {
                       let #output = #input.squeeze::<#output_rank>();
                   }
               }
           }
       }

       fn into_node(self) -> Node<PS> {
           Node::Squeeze(self)
       }
   }
   ```

4. Add unit tests in the same file to verify the generated code compiles and works correctly. These
   tests typically call a helper function like `assert_tokens()` to validate the generated code
   against expected output.

### Step 3: Register in Module System

Add the module declaration to `crates/burn-import/src/burn/node/mod.rs`:

```rust
// ... other node modules
pub(crate) mod squeeze;
// ... more node modules
```

The modules are automatically made visible through re-exports in the same file.

### Step 4: Register in Node Registry

Add your operation to the node registry in `crates/burn-import/src/burn/node_registry.rs`. This is
the **single source of truth** for all ONNX node conversions.

For a single ONNX operation mapping to a single node type:

```rust
node_registry! {
    // ... other operations
    Squeeze => squeeze as SqueezeNode,
    // ... more operations
}
```

For multiple ONNX operations mapping to the same node type (e.g., various reduce operations):

```rust
node_registry! {
    // ... other operations
    [ReduceMax, ReduceMin, ReduceMean, ReduceProd, ReduceSum]
        => ReduceMax: reduce as ReduceNode,
    // ... more operations
}
```

That's it! The registry automatically generates:

- The `Node<PS>` enum with your operation as a variant
- The `match_all!` macro for dispatching
- The ONNX to Burn conversion logic
- All necessary imports

### Step 5: Processor Implementation

The `NodeProcessor` trait defines how operations are processed in onnx-ir. Each processor must
implement:

1. **Associated type**: `type Config` - Define your configuration struct (use `()` if no config)
2. **`infer_types()`** - Infer output types from inputs and config (required)
3. **`build_node()`** - Construct the `Node` enum variant (required)
4. **`extract_config()`** - Extract config from attributes/inputs (override if Config != `()`)
5. **`spec()`** - Define opset and input/output requirements (optional)
6. **`lift_constants()`** - Request constant lifting for inputs (optional)

For complete examples, see existing processors:

- **Simple operation**: `crates/onnx-ir/src/node/softmax.rs`
- **With constant inputs**: `crates/onnx-ir/src/node/squeeze.rs`
- **Complex operation**: `crates/onnx-ir/src/node/conv2d.rs`

See [NodeProcessor Trait](#nodeprocessor-trait) for the complete trait definition.

### Step 6: Add Newly Supported Op!

As a reward, add an extra check to `crates/burn-import/SUPPORTED-ONNX-OPS.md`!

### Constant Lifting

The onnx-ir pipeline automatically handles constant lifting during the post-processing phase.
"Lifting" constants means making constant values directly accessible on node inputs via
`Argument::value()`, instead of requiring a separate graph traversal to find a Constant node.

**When to use**: If your operation takes constant inputs (e.g., weights in Conv1d, shape tensors in
Reshape, axes in Squeeze), access them via `node.inputs[N].value()` in your `extract_config()`
method. See the [Configuration Extraction example](#example-configuration-extraction) in Step 5.

**Optional optimization**: Implement `lift_constants()` to explicitly request constant lifting for
specific inputs before `extract_config()` is called. The pipeline handles this automatically during
post-processing.

## Architecture Overview

### ONNX-IR Pipeline

The `onnx-ir` crate converts ONNX models to an Intermediate Representation through a 5-phase
pipeline:

#### Phase 1: Initialization

- Creates `GraphState` from ONNX proto structures
- **Constants-first approach**: Converts all ONNX initializers into Constant nodes, providing a
  uniform starting point for processing
- Sets up the value store for tensor data using `burn_tensor::TensorData`
- Preserves sanitized graph input names for debugging

#### Phase 2: Node Conversion

- Converts ONNX nodes to IR nodes using registered processors
- Creates `NodeBuilder` instances from ONNX proto nodes
- Processors extract configuration and construct typed `Node` enum variants
- Handles constant nodes specially (extracting values from attributes into tensor store)
- Each processor is responsible for its own type inference and node construction

#### Phase 3: Type Inference

- Type inference happens within each processor's `process()` method during Phase 2
- Processors infer output types based on input types and configuration
- Multi-pass processing handles dependencies between nodes
- The pipeline may need multiple iterations for complex type dependencies (e.g., control flow)

#### Phase 4: Post-processing

- Lifts constants: Makes constant values accessible on downstream node inputs
- Eliminates Identity nodes: Removes no-op nodes and rewires the graph
- Re-runs constant lifting after Identity elimination

#### Phase 5: Finalization

- Removes unreferenced constant nodes
- Constructs the final `OnnxGraph` with inputs, outputs, and nodes

### NodeProcessor Trait

The `NodeProcessor` trait (defined in `crates/onnx-ir/src/processor.rs`) is the core abstraction for
handling ONNX operations. Each processor implements:

**Required:**

- `type Config` - Associated type for configuration (use `()` if no config needed)
- `infer_types()` - Infer output types from inputs and configuration
- `build_node()` - Construct the final `Node` enum variant

**Optional (have defaults):**

- `spec()` - Define opset requirements and input/output count validation (`NodeSpec`, `InputSpec`,
  `OutputSpec`)
- `extract_config()` - Extract configuration from attributes/inputs (default returns
  `Default::default()`)
- `lift_constants()` - Request constant lifting for specific inputs (default does nothing)
- `input_preferences()` - Declare preferred input types from producers (default returns `None`)

Design principles: Each processor is self-contained, handling type inference, config extraction, and
node construction. Processors return strongly-typed `Node` enum variants, ensuring type safety
throughout the pipeline.

## Testing

When implementing a new operator, there are several levels of testing to consider:

### Unit Testing

- **Processor Methods**: Write unit tests in `crates/onnx-ir/src/node/<operation_name>.rs` to
  verify:
  - `extract_config()` - Correctly extracts configuration from attributes and inputs
  - `infer_types()` - Correctly infers output types (element type, rank, static shapes)
  - `build_node()` - Constructs correct `Node` enum variant
  - `spec()` - Defines correct opset and input/output requirements
  - Error handling for invalid inputs or configurations

  See existing tests in `crates/onnx-ir/src/node/squeeze.rs` for examples.

- **Code Generation**: Test the burn-import Node implementation to verify correct Rust code
  generation. Each node file typically includes unit tests using `assert_tokens()` to validate
  generated code against expected output.

### Integration Testing

- Create small ONNX models that use your operator and test the end-to-end conversion process
- Ensure the generated Rust code compiles and produces the expected outputs
- Add these tests to `crates/burn-import/onnx-tests/tests/test_onnx.rs`

### End-to-End Testing

- Test with realistic ONNX models that use your operator in conjunction with others
- Verify that inputs and outputs match between the original ONNX model and the converted Burn model
- Include models that test edge cases (e.g., different input shapes, parameter combinations)

Testing the processor implementation is particularly important as it directly affects the
correctness of the conversion process. Incorrect type inference can lead to mismatched tensor shapes
or wrong element types, while incorrect configuration extraction can cause runtime errors or produce
incorrect results.

## Node Enum Architecture

The ONNX-IR uses an enum-based node representation where each ONNX operation is a variant of the
`Node` enum (defined in `crates/onnx-ir/src/ir/node.rs`). Each variant contains `name`, `inputs`,
`outputs`, and optionally an operation-specific `config` struct. The `define_node_enum!` macro
generates both the `NodeType` (simple enum for parsing) and `Node` (enum with configs) from a single
source, ensuring they stay in sync. This provides type safety and makes it easy to pattern match on
specific operations to access their configuration.

## Resources

1. [PyTorch to ONNX](https://pytorch.org/docs/stable/onnx.html)
2. [ONNX to PyTorch](https://github.com/ENOT-AutoDL/onnx2torch)
3. [ONNX Introduction](https://onnx.ai/onnx/intro/)
4. [ONNX Operators](https://onnx.ai/onnx/operators/index.html)
5. [ONNX Protos](https://onnx.ai/onnx/api/classes.html)
6. [ONNX Optimizer](https://github.com/onnx/optimizer)
7. [Netron](https://github.com/lutzroeder/netron)
